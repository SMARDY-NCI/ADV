---
title: "Dublin council dataset"
author: "NCI - Smardy"
date: '2022-05-04'
output: html_document
editor_options: 
  markdown: 
    wrap: sentence
---

# Introduction

This report contains the results of the experiments run for different data versioning approaches.
The interest of data versioning is closely related to the FAIR principles of data:

-   Interoperability principle I3.
    (Meta)data include qualified references to other (meta)data.
    Cross-reference that explains as many meaningful links as possible between (meta)data resources to enrich the contextual knowledge about the data.
    To be more concrete, you should specify if one dataset builds on another data set, if additional datasets are needed to complete the data, or if complementary information is stored in a different dataset.
    In particular, the scientific links between the datasets need to be described.
    Furthermore, all datasets need to be properly cited (i.e., including their globally unique and persistent identifiers).

-   Reusability principle R2.
    R1.2.
    (Meta)data are associated with detailed provenance.
    There should be a clear story of the workflow that finally lead to the used data: Who generated or collected it?
    How has it been processed?
    Has it been published before?
    Does it contain data from someone else that you may have transformed or completed?
    Ideally, this workflow is described in a machine-readable format.

As far as we ar concerned, there is not a clear consensus on how to address the aforementioned issues.
How to measure the relations between linked databases and what terms use for the definition of that relationships, are questions that remain open about the mature implantation of the FAIR principles.
Nonetheless, the benefits of having that information available would be undeniable, specially in the context of scientific and research data, where bibliography review and comparison could be much more insightful and objective if based on a standard system of data versioning.

In this work, we propose and compare two different strategies as the core of a data versioning algorithm.
The goal of these techniques is to extract statistical parameters about datasets in such a way that they inform about the degree of difference between a new version and the original one.
To do so, we apply the philosophy of dimensionality reduction as a way to extract these relevant features.
These dimensionality reduction techniques are known for their compression power, using them frequently as denoising strategies.
Thus, it seems reasonable to expect that the parameters of that lower-dimensionality space, would change accordingly to the degree of change on the dataset structure, while staying safe of unsubstantial changes.

# Methodology

Two different approaches are compared in this report: the parameters extracted by a Principal Component Analysis (PCA) model and by a Vanilla Autoencoder.
This set of parameters would be integrated as part of the metadata, enabling a quantitative and standard framework for data comparison that could be performed based on a set of metadata fields.
Some further steps would include the definition of versioning tags based on such set of parameters and the implications on the comparability between data objects.
In this report, the outcomes of both approaches are tested on an open data repository about Dublin city footfall counts from January 1st to April 3rd 2022.
It is time series data that also present some technical particularities, such as the subsetting specific time windows or the transformation of variables to another units.
Nonetheless, time series data are also frequent as the type of data registered in the Internet Of Things paradigm.
Hence, we consider that although this work focuses on this particular nature of the data, it is still a relevant and significant problem encountered in the data science community.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, 
                      out.width='50%', fig.align = "center",
                      message=FALSE)

library(dplyr)
library(magrittr)
library(factoextra)
library(qqplotr)
library(tictoc)
library(caret)
library(keras)
library(ggplot2)
library(tidyr)
library(tensorflow)
source(here::here("R","pcals.R"))
source(here::here("R","autoencoder.R"))
source(here::here("R","pca_functions.R"))
source(here::here("R","pcambtsrR.R"))
source(here::here("R","lsdAnalysis.R"))
source(here::here("R","lsdfig.R"))
```

Load the data:

```{r load data}
dat <- readxl::read_xlsx(here::here("data","AirQualityUCI.xlsx")) 
dat[dat==-200] <- NA
dat.time <- dat
dat.time <- dat.time %>% filter(complete.cases(.))
dat <- dat %>% select_if(is.numeric) %>% filter(complete.cases(.))
```

We check if there are variables with null variance that should be deleted:

```{r delnullvar, out.width='70%'}
print("These variables show null variance. Consider their deletion before the PCA")
summary(dat[,(apply(dat,2,var)==0),drop=F])
dat.or <- dat
dat <- dat[,!(apply(dat,2,var)==0),drop=F]
```

Moreover, a quick exploratory data analysis is performed to study the distribution and features of the variables in the dataset, as well as the existence of phenomena such as missing data or subpopulations.

```{r univanalysis}
summary(dat)
boxplot(dat,
data=dat,
main="Air Quality",
xlab="Parameters",
ylab="",
col="green3",
border="black"
)
dat %>% gather() %>% head()

ggplot(gather(dat), aes(value, after_stat(density))) + 
    geom_histogram(bins = 30) + 
    facet_wrap(~key, scales = 'free') + 
	theme(strip.text.x = element_text(size = 6))

p <- ggplot(dat, aes(y=value)) +
  geom_line() + 
  xlab("")

library(gridExtra)
lapply(1:ncol(dat),function(x){
  p1 <- ggplot(data = dat, mapping = aes(sample = dat[,x])) +
  stat_qq_band(fill="blue",alpha = 0.15) +
  stat_qq_line(col="blue") +
  stat_qq_point(col="black",size=1)+
  labs(title = colnames(dat)[x])+theme_bw()
  
  p2 <- ggplot(data = dat, mapping = aes(sample = dat[,x])) +
  stat_qq_band(fill="green",alpha = 0.15) +
  stat_qq_line(col="green") +
  stat_qq_point(col="black",size=1)+
  labs(title = colnames(dat)[x])+theme_bw()
  
  p1p2 <- grid.arrange(p1,p2,ncol=2)
  p1p2})

```

# Results

## Reference models

The first thing to do is to compute the reference models.
These would yield the set of parameters characterizing the very initial version of the dataset.
At this stage, an hyperparameter optimization should be carried out to determine the optimal model architecture, that is, the number of the latent variables to be extracted, the activation functions to be used on the autoencoder neurons, etc.
The loss function used to set the optimal model, will be the Mean Squared Error, computed as the average squared reconstruction error obtained on the original data matrix.

```{r optmodels, echo=FALSE}
pca.df <- stats::prcomp(dat,center = TRUE,scale = TRUE)
pca.df.VE<- (pca.df$sdev^2)/sum((pca.df$sdev^2))*100
pca.df.CumVE<- cumsum(pca.df.VE)
pca.DF <- data.frame(rbind(pca.df.CumVE))
writexl::write_xlsx(pca.DF,"reports\\Variance Explained AQ.xlsx")
# tic("Autoencoder optimization")
# opt.AE.results <- autoenc_opt(dat, A.values = c(1:10), tr = NULL, kcv = 10, act.fun = "relu", n.epochs = 50) 
# time.EA.opt <- toc()

# tic("PCA optimization")
# opt.PCA.results <- pca_opt(dat, A.values = c(1:10), tr = opt.AE.results$tr, kcv = 10) 
# time.PCA.opt <- toc()
load("../AQ_optAElayers_relu.RData")
df.anova.test <- as.data.frame(c(as.vector(opt.AE.results$lossts[1:10,]), 
                                     as.vector(opt.PCA.results$lossts)))
colnames(df.anova.test) <- "MSE"
df.anova.test$Method <- rep(c("Autoencoder", "PCA"),each = 10*10)
df.anova.test$NLVs <- as.factor(rep(c(1:10),times = 2*10))
df.anova.test$Repetition <- as.factor(rep(rep(c(1:10),each = 10), times=2))
lsd.MSE <- aov(MSE ~ Method + NLVs + Repetition, data = df.anova.test)
summary(lsd.MSE)

df.anova.test.log <- df.anova.test
df.anova.test.log$MSE <- log10(df.anova.test$MSE)
lsd.log.MSE <- aov(MSE ~ Method + NLVs + Repetition, data = df.anova.test.log)
summary(lsd.log.MSE)

mse.lsd.log <- sum(lsd.log.MSE$residuals^2)/(lsd.log.MSE$df.residual)
lsd.width <- sqrt(mse.lsd.log*2/length(unique(df.anova.test.log$Repetition)))*
  (qt(1-0.025,lsd.log.MSE$df.residual))
lsdfig(df.anova.test.log,"MSE","NLVs", lsd.width, col=rgb(0,1,0,0.5), ytext="log10 (MSE)", 
       xtext="Latent dimension", tittext = "Reference models")
```

Only the PCA model is showing a noticeable decay in the Mean Squared Error of the reconstructed matrices.
A number of four latent variables (4 PCs in the PCA case and 4 units in the latent layer of the Autoencoder) is chosen.
This selection is based on the fact that from 4 latent variables, the MSE decay slows down for the PCA model, yielding to almost non-statistically significant differences within the range between four and eight PCs.

```{r refmodels}
if(file.exists("../AQref_models.RData") & file.exists(file="../AQref_loadings.RData")){
  load("../AQref_models.RData")
  load(file="../AQref_loadings.RData")
} else {
  # model.AE <- fit_autoencoder(dat, 4, "relu")
  # plot(model.AE$trainhist)
  model.PCA <- fit_pca(dat, 9, xscale = F)
  P.ae.ref <- as.matrix(model.AE$model$layers[[2]]$weights[[1]])
  P.pca.ref <- model.PCA$model$rotation
  save(list = c("P.ae.ref", "P.pca.ref"),file="AQref_loadings.RData")
  save(list = c("model.AE", "model.PCA"),file="AQref_models.RData")
}
```

## Versioning scenario 1: Missing data imputation

```{r mdimputation,echo=FALSE,out.width="50%",fig.asp=0.9}
load("../AQ_MissingData_models.RData")
pca_remcells$para_test$Method <- "PCA"
autoencoder_remcells$para_test$Method <- "AutoEncoder"
mdi.exp.data <- rbind(pca_remcells$para_test, autoencoder_remcells$para_test)
mdi.exp.data$Method <- as.factor(mdi.exp.data$Method)
mdi.exp.data$MDpctge <- as.factor(mdi.exp.data$MDpctge)
mdi.exp.data$Repetition <- as.factor(mdi.exp.data$Repetition)
mdi.exp.data <- mdi.exp.data[, -which(grepl("^t.",colnames(mdi.exp.data)))]
mdi.exp.data <- mdi.exp.data[, -which(grepl("^lim.",colnames(mdi.exp.data)))]
mdi.exp.data <- mdi.exp.data[, -which(grepl("^log.",colnames(mdi.exp.data)))]

lsd.results <- lsdAnalysis(mdi.exp.data, factor1 = "Method", factor2 = "MDpctge", 
													 factorrnd = "Repetition", factor.color = "Method", 
													 graph.out = "curve", hier = T, xtext = "missing cells (%)")
plot.results <- lsd.results$l.plots[grepl("p*corr", names(lsd.results$l.plots))]
mdi.plots <- ggpubr::ggarrange(plotlist = plot.results, ncol = 3, nrow = 1,
                  font.label = list(size = 3, color = "black", face = "bold", family = NULL),
                  common.legend = TRUE)
DF.exp1.plots <- ggpubr::annotate_figure(mdi.plots, top = ggpubr::text_grob("Case I: Missing Data Imputation - Air Quality", color = "black", face = "bold", size = 12))
DF.exp1.plots
```

## Versioning scenario 2: Transformation of variables

### Rowwise transformation

```{r transform, results="hide", echo=FALSE,out.width="50%",fig.asp=0.9}
load("../RowsPctge_models.RData")
pca_transvars$para_test$Method <- "PCA"
autoencoder_transvars$para_test$Method <- "AutoEncoder"
exp2.data <- rbind(pca_transvars$para_test, autoencoder_transvars$para_test)
exp2.lsd.results <- lsdAnalysis(exp2.data, factor1 = "Method", factor.color = "Method", 
																hier = F)
plot.results.exp2 <- exp2.lsd.results$l.plots[grepl("p*corr", names(exp2.lsd.results$l.plots))]
exp2.plots <- ggpubr::ggarrange(plotlist = plot.results.exp2, ncol = 2, nrow = 2,
                  font.label = list(size = 3, color = "black", face = "bold", family = NULL),
                  common.legend = TRUE)
DF.exp2.plots <- ggpubr::annotate_figure(exp2.plots, top = ggpubr::text_grob("Case II: Rowwise transformation - Dublin Footfall", color = "black", face = "bold", size = 12))
DF.exp2.plots
```

### Columnwise transformation

```{r coxtransforn, results='hide', echo=FALSE, message=FALSE}
load("../AQ_coltrans_models.RData")
load("../AQ_coltrans_models.RData")
pca_transvars$para_test$Method <- "PCA"
autoencoder_transvars$para_test$Method <- "AutoEncoder"
exp3.data <- rbind(pca_transvars$para_test, autoencoder_transvars$para_test)
exp3.data$Method <- as.factor(exp3.data$Method)
exp3.data$Transpctge <- as.factor(rep(rep(c(5,10,seq(20,80,by=20)), each = 20), times = 2))
exp3.data$Repetition <- as.factor(exp3.data$Repetition)
exp3.data <- exp3.data[, -which(grepl("^t.",colnames(exp3.data)))]
exp3.data <- exp3.data[, -which(grepl("^lim.",colnames(exp3.data)))]
exp3.data <- exp3.data[, -which(grepl("^log.",colnames(exp3.data)))]


exp3.lsd.results <- lsdAnalysis(exp3.data, factor1 = "Method", factor2 = "Coltrans", factor.color = "Method", graph.out = "curve")
plot.results.exp3 <- exp3.lsd.results$l.plots[grepl("p*corr", names(exp3.lsd.results$l.plots))]
exp3.plots <- ggpubr::ggarrange(plotlist = plot.results.exp3$l.plots, ncol = 2, nrow = 2,
                  font.label = list(size = 3, color = "black", face = "bold", family = NULL),
                  common.legend = TRUE)
DF.exp3.plots <- ggpubr::annotate_figure(exp3.mdi.plots, top = ggpubr::text_grob("Case III: Column-wise transformation - Dublin Footfall",
                                       color = "black", face = "bold", size = 12))
DF.exp3.plots
```

## Versioning scenario 3: Change in the number of rows

```{r remrows, results="hide",out.width="50%",fig.asp=0.9}
load("../RemRows_models.RData")
pca_remrows$para_test$Method <- "PCA"
autoencoder_remrows$para_test$Method <- "AutoEncoder"
exp4.data <- rbind(pca_remrows$para_test[,-c(1,2)], autoencoder_remrows$para_test)
exp4.lsd.results <- lsdAnalysis(exp4.data, factor1 = "Method", factor2 = "RWoutpctge", 
													 factor.color = "Method", graph.out = "curve", hier = T, xtext = "deleted rows (%)")
plot.results.exp4 <- exp4.lsd.results$l.plots[grepl("p*corr", names(exp4.lsd.results$l.plots))]
exp4.mdi.plots <- ggpubr::ggarrange(plotlist = plot.results.exp4, ncol = 2, nrow = 2,
                  font.label = list(size = 3, color = "black", face = "bold", family = NULL),
                  common.legend = TRUE)
DF.exp4.plots <- ggpubr::annotate_figure(exp4.mdi.plots, top = ggpubr::text_grob("Case III: Subsetting rows - Dublin Footfall",
                                       color = "black", face = "bold", size = 12))
```

## Versioning scenario 4: Change in the number of variables

```{r remvars, results = "hide"}

```

# Conclusions

# References

    @misc{chollet2017kerasR,
      title={R Interface to Keras},
      author={Chollet, Fran\c{c}ois and Allaire, JJ and others},
      year={2017},
      publisher={GitHub},
      howpublished={\url{https://github.com/rstudio/keras}},
    }
