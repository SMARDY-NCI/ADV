
Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

Loading required package: ggplot2
Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa

Attaching package: ‘qqplotr’

The following objects are masked from ‘package:ggplot2’:

    stat_qq_line, StatQqLine

Loading required package: lattice

Attaching package: ‘tidyr’

The following object is masked from ‘package:magrittr’:

    extract


Attaching package: ‘tensorflow’

The following object is masked from ‘package:caret’:

    train

[1] "These variables show null variance. Consider their deletion before the PCA"
 Talbot.st.Murrays.Pharmacy Talbot.st.Murrays.Pharmacy.IN
 Min.   :0                  Min.   :0                    
 1st Qu.:0                  1st Qu.:0                    
 Median :0                  Median :0                    
 Mean   :0                  Mean   :0                    
 3rd Qu.:0                  3rd Qu.:0                    
 Max.   :0                  Max.   :0                    
 Talbot.st.Murrays.Pharmacy.OUT
 Min.   :0                     
 1st Qu.:0                     
 Median :0                     
 Mean   :0                     
 3rd Qu.:0                     
 Max.   :0                     
                           key value
1 Bachelors.walk.Bachelors.way   887
2 Bachelors.walk.Bachelors.way  1016
3 Bachelors.walk.Bachelors.way   204
4 Bachelors.walk.Bachelors.way   132
5 Bachelors.walk.Bachelors.way    57
6 Bachelors.walk.Bachelors.way    40
[1] "N layers at  30"
2022-05-26 11:33:19.796341: I tensorflow/core/util/util.cc:159] Experimental oneDNN custom operations are on. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
Loaded Tensorflow version 2.8.0
2022-05-26 11:33:21.734750: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/R/lib:/usr/lib/x86_64-linux-gnu:/usr/lib/jvm/default-java/lib/server
2022-05-26 11:33:21.734779: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-05-26 11:33:21.734803: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (phi): /proc/driver/nvidia/version does not exist
2022-05-26 11:33:21.735039: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "sequential"
________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     
================================================================================
 input (Dense)                      (None, 30)                      930         
                                                                                
 latent (Dense)                     (None, 4)                       124         
                                                                                
 output (Dense)                     (None, 30)                      150         
                                                                                
================================================================================
Total params: 1,204
Trainable params: 1,204
Non-trainable params: 0
________________________________________________________________________________
1/7 [===>..........................] - ETA: 0s - loss: 831433.56257/7 [==============================] - 0s 6ms/step - loss: 1663377.6250
1/1 [==============================] - ETA: 0s - loss: 1659432.00001/1 [==============================] - 0s 22ms/step - loss: 1659432.0000
1/2 [==============>...............] - ETA: 0s - loss: 1381424.62502/2 [==============================] - 0s 7ms/step - loss: 1574382.2500
1/7 [===>..........................] - ETA: 0s - loss: 846239.18757/7 [==============================] - 0s 6ms/step - loss: 1679456.1250
1/1 [==============================] - ETA: 0s - loss: 1517552.37501/1 [==============================] - 0s 22ms/step - loss: 1517552.3750
1/2 [==============>...............] - ETA: 0s - loss: 1381455.87502/2 [==============================] - 0s 6ms/step - loss: 1575575.2500
1/7 [===>..........................] - ETA: 0s - loss: 853671.12507/7 [==============================] - 0s 6ms/step - loss: 1667445.5000
1/1 [==============================] - ETA: 0s - loss: 1623673.37501/1 [==============================] - 0s 23ms/step - loss: 1623673.3750
1/2 [==============>...............] - ETA: 0s - loss: 1380814.75002/2 [==============================] - 0s 6ms/step - loss: 1573889.0000
1/7 [===>..........................] - ETA: 0s - loss: 850177.37507/7 [==============================] - 0s 6ms/step - loss: 1630874.7500
1/1 [==============================] - ETA: 0s - loss: 1962377.00001/1 [==============================] - 0s 22ms/step - loss: 1962377.0000
1/2 [==============>...............] - ETA: 0s - loss: 1382064.50002/2 [==============================] - 0s 6ms/step - loss: 1573711.3750
1/7 [===>..........................] - ETA: 0s - loss: 828229.87507/7 [==============================] - 0s 6ms/step - loss: 1637359.0000
1/1 [==============================] - ETA: 0s - loss: 1890757.62501/1 [==============================] - 0s 22ms/step - loss: 1890757.6250
1/2 [==============>...............] - ETA: 0s - loss: 1381080.25002/2 [==============================] - 0s 6ms/step - loss: 1575315.7500
1/7 [===>..........................] - ETA: 0s - loss: 896061.43757/7 [==============================] - 0s 6ms/step - loss: 1650307.5000
1/1 [==============================] - ETA: 0s - loss: 1774983.62501/1 [==============================] - 0s 22ms/step - loss: 1774983.6250
1/2 [==============>...............] - ETA: 0s - loss: 1382734.00002/2 [==============================] - 0s 6ms/step - loss: 1574770.5000
1/7 [===>..........................] - ETA: 0s - loss: 888567.50007/7 [==============================] - 0s 6ms/step - loss: 1678591.1250
1/1 [==============================] - ETA: 0s - loss: 1517585.87501/1 [==============================] - 0s 21ms/step - loss: 1517585.8750
1/2 [==============>...............] - ETA: 0s - loss: 1381978.25002/2 [==============================] - 0s 8ms/step - loss: 1574360.5000
1/7 [===>..........................] - ETA: 0s - loss: 979670.75007/7 [==============================] - 0s 6ms/step - loss: 1686320.1250
1/1 [==============================] - ETA: 0s - loss: 1455743.37501/1 [==============================] - 0s 22ms/step - loss: 1455743.3750
1/2 [==============>...............] - ETA: 0s - loss: 1383119.00002/2 [==============================] - 0s 6ms/step - loss: 1574753.8750
1/7 [===>..........................] - ETA: 0s - loss: 853300.62507/7 [==============================] - 0s 6ms/step - loss: 1668581.5000
1/1 [==============================] - ETA: 0s - loss: 1612810.12501/1 [==============================] - 0s 22ms/step - loss: 1612810.1250
1/2 [==============>...............] - ETA: 0s - loss: 1381237.00002/2 [==============================] - 0s 6ms/step - loss: 1574396.8750
1/7 [===>..........................] - ETA: 0s - loss: 853563.75007/7 [==============================] - 0s 6ms/step - loss: 1667585.7500
1/1 [==============================] - ETA: 0s - loss: 1622208.62501/1 [==============================] - 0s 22ms/step - loss: 1622208.6250
1/2 [==============>...............] - ETA: 0s - loss: 1382293.25002/2 [==============================] - 0s 6ms/step - loss: 1574424.0000
[1] "N layers at  16"
Model: "sequential_1"
________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     
================================================================================
 input (Dense)                      (None, 30)                      930         
                                                                                
 hidd_in_16 (Dense)                 (None, 16)                      496         
                                                                                
 latent (Dense)                     (None, 4)                       68          
                                                                                
 hidd_out_16 (Dense)                (None, 16)                      80          
                                                                                
 output (Dense)                     (None, 30)                      510         
                                                                                
================================================================================
Total params: 2,084
Trainable params: 2,084
Non-trainable params: 0
________________________________________________________________________________
1/7 [===>..........................] - ETA: 0s - loss: 63987.70707/7 [==============================] - 0s 7ms/step - loss: 185960.0312
1/1 [==============================] - ETA: 0s - loss: 205595.60941/1 [==============================] - 0s 23ms/step - loss: 205595.6094
1/2 [==============>...............] - ETA: 0s - loss: 162045.03122/2 [==============================] - 0s 7ms/step - loss: 234844.7500
1/7 [===>..........................] - ETA: 0s - loss: 70476.93757/7 [==============================] - 0s 7ms/step - loss: 176511.4375
1/1 [==============================] - ETA: 0s - loss: 143549.29691/1 [==============================] - 0s 26ms/step - loss: 143549.2969
1/2 [==============>...............] - ETA: 0s - loss: 155699.00002/2 [==============================] - 0s 7ms/step - loss: 226264.2344
1/7 [===>..........................] - ETA: 0s - loss: 65577.59387/7 [==============================] - 0s 7ms/step - loss: 135716.4062
1/1 [==============================] - ETA: 0s - loss: 109959.43751/1 [==============================] - 0s 22ms/step - loss: 109959.4375
1/2 [==============>...............] - ETA: 0s - loss: 113546.10162/2 [==============================] - 0s 8ms/step - loss: 166472.2031
1/7 [===>..........................] - ETA: 0s - loss: 59575.47277/7 [==============================] - 0s 7ms/step - loss: 124034.3047
1/1 [==============================] - ETA: 0s - loss: 163102.39061/1 [==============================] - 0s 23ms/step - loss: 163102.3906
1/2 [==============>...............] - ETA: 0s - loss: 110360.08592/2 [==============================] - 0s 7ms/step - loss: 158709.6250
1/7 [===>..........................] - ETA: 0s - loss: 60272.82817/7 [==============================] - 0s 7ms/step - loss: 126891.6172
1/1 [==============================] - ETA: 0s - loss: 126401.53911/1 [==============================] - 0s 23ms/step - loss: 126401.5391
1/2 [==============>...............] - ETA: 0s - loss: 116457.29692/2 [==============================] - 0s 6ms/step - loss: 166521.4844
1/7 [===>..........................] - ETA: 0s - loss: 69444.59387/7 [==============================] - 0s 7ms/step - loss: 126310.7734
1/1 [==============================] - ETA: 0s - loss: 112311.83591/1 [==============================] - 0s 23ms/step - loss: 112311.8359
1/2 [==============>...............] - ETA: 0s - loss: 116384.25002/2 [==============================] - 0s 8ms/step - loss: 164535.5625
1/7 [===>..........................] - ETA: 0s - loss: 63144.68757/7 [==============================] - 0s 7ms/step - loss: 123421.9844
1/1 [==============================] - ETA: 0s - loss: 127994.54691/1 [==============================] - 0s 23ms/step - loss: 127994.5469
1/2 [==============>...............] - ETA: 0s - loss: 111797.54692/2 [==============================] - 0s 7ms/step - loss: 162521.1562
1/7 [===>..........................] - ETA: 0s - loss: 67286.70317/7 [==============================] - 0s 7ms/step - loss: 117429.3828
1/1 [==============================] - ETA: 0s - loss: 153237.92191/1 [==============================] - 0s 25ms/step - loss: 153237.9219
1/2 [==============>...............] - ETA: 0s - loss: 116672.34382/2 [==============================] - 0s 7ms/step - loss: 162939.8281
1/7 [===>..........................] - ETA: 0s - loss: 65220.48447/7 [==============================] - 0s 7ms/step - loss: 120799.5391
1/1 [==============================] - ETA: 0s - loss: 110228.75781/1 [==============================] - 0s 23ms/step - loss: 110228.7578
1/2 [==============>...............] - ETA: 0s - loss: 114207.28912/2 [==============================] - 0s 8ms/step - loss: 160359.0625
1/7 [===>..........................] - ETA: 0s - loss: 59072.29697/7 [==============================] - 0s 7ms/step - loss: 118598.1953
1/1 [==============================] - ETA: 0s - loss: 108694.66411/1 [==============================] - 0s 22ms/step - loss: 108694.6641
1/2 [==============>...............] - ETA: 0s - loss: 114012.11722/2 [==============================] - 0s 7ms/step - loss: 164963.8594
[1] "N layers at  8"
Model: "sequential_2"
________________________________________________________________________________
 Layer (type)                       Output Shape                    Param #     
================================================================================
 input (Dense)                      (None, 30)                      930         
                                                                                
 hidd_in_16 (Dense)                 (None, 16)                      496         
                                                                                
 hidd_in_8 (Dense)                  (None, 8)                       136         
                                                                                
 latent (Dense)                     (None, 4)                       36          
                                                                                
 hidd_out_8 (Dense)                 (None, 8)                       40          
                                                                                
 hidd_out_16 (Dense)                (None, 16)                      144         
                                                                                
 output (Dense)                     (None, 30)                      510         
                                                                                
================================================================================
Total params: 2,292
Trainable params: 2,292
Non-trainable params: 0
________________________________________________________________________________
1/7 [===>..........................] - ETA: 0s - loss: 112185.28127/7 [==============================] - 0s 7ms/step - loss: 263590.1250
1/1 [==============================] - ETA: 0s - loss: 278354.96881/1 [==============================] - 0s 23ms/step - loss: 278354.9688
1/2 [==============>...............] - ETA: 0s - loss: 199731.95312/2 [==============================] - 0s 7ms/step - loss: 269970.6562
1/7 [===>..........................] - ETA: 0s - loss: 107941.46887/7 [==============================] - 0s 8ms/step - loss: 255351.4688
1/1 [==============================] - ETA: 0s - loss: 253288.00001/1 [==============================] - 0s 31ms/step - loss: 253288.0000
1/2 [==============>...............] - ETA: 0s - loss: 184037.71882/2 [==============================] - 0s 9ms/step - loss: 257310.8594
1/7 [===>..........................] - ETA: 0s - loss: 111467.56257/7 [==============================] - 0s 7ms/step - loss: 251227.4062
1/1 [==============================] - ETA: 0s - loss: 221736.75001/1 [==============================] - 0s 27ms/step - loss: 221736.7500
1/2 [==============>...............] - ETA: 0s - loss: 182143.56252/2 [==============================] - 0s 7ms/step - loss: 251990.2969
1/7 [===>..........................] - ETA: 0s - loss: 112555.76567/7 [==============================] - 0s 7ms/step - loss: 245388.6094
1/1 [==============================] - ETA: 0s - loss: 277353.75001/1 [==============================] - 0s 30ms/step - loss: 277353.7500
1/2 [==============>...............] - ETA: 0s - loss: 183837.90622/2 [==============================] - 0s 8ms/step - loss: 253623.5469
1/7 [===>..........................] - ETA: 0s - loss: 105956.25007/7 [==============================] - 0s 7ms/step - loss: 245841.2344
1/1 [==============================] - ETA: 0s - loss: 259910.45311/1 [==============================] - 0s 23ms/step - loss: 259910.4531
1/2 [==============>...............] - ETA: 0s - loss: 181973.31252/2 [==============================] - 0s 7ms/step - loss: 249649.3594
1/7 [===>..........................] - ETA: 0s - loss: 111315.00007/7 [==============================] - 0s 7ms/step - loss: 249673.8281
1/1 [==============================] - ETA: 0s - loss: 269919.00001/1 [==============================] - 0s 23ms/step - loss: 269919.0000
1/2 [==============>...............] - ETA: 0s - loss: 186101.67192/2 [==============================] - 0s 8ms/step - loss: 260629.0156
1/7 [===>..........................] - ETA: 0s - loss: 107348.04697/7 [==============================] - 0s 7ms/step - loss: 240912.9688
1/1 [==============================] - ETA: 0s - loss: 224599.15621/1 [==============================] - 0s 24ms/step - loss: 224599.1562
1/2 [==============>...............] - ETA: 0s - loss: 181001.95312/2 [==============================] - 0s 9ms/step - loss: 255914.6406
1/7 [===>..........................] - ETA: 0s - loss: 122318.02347/7 [==============================] - 0s 7ms/step - loss: 236103.2500
1/1 [==============================] - ETA: 0s - loss: 242027.35941/1 [==============================] - 0s 28ms/step - loss: 242027.3594
1/2 [==============>...............] - ETA: 0s - loss: 179613.71882/2 [==============================] - 0s 9ms/step - loss: 252266.5156
1/7 [===>..........................] - ETA: 0s - loss: 103150.32817/7 [==============================] - 0s 8ms/step - loss: 239766.2188
1/1 [==============================] - ETA: 0s - loss: 214794.10941/1 [==============================] - 0s 28ms/step - loss: 214794.1094
1/2 [==============>...............] - ETA: 0s - loss: 177126.39062/2 [==============================] - 0s 8ms/step - loss: 250780.3125
1/7 [===>..........................] - ETA: 0s - loss: 102992.46887/7 [==============================] - 0s 8ms/step - loss: 238034.7969
1/1 [==============================] - ETA: 0s - loss: 221150.50001/1 [==============================] - 0s 25ms/step - loss: 221150.5000
1/2 [==============>...............] - ETA: 0s - loss: 180038.62502/2 [==============================] - 0s 8ms/step - loss: 254551.8281
